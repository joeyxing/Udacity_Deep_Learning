{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (400000, 28, 28) (400000,)\n",
      "Validation set (20000, 28, 28) (20000,)\n",
      "Test set (15000, 28, 28) (15000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (400000, 784) (400000, 10)\n",
      "Validation set (20000, 784) (20000, 10)\n",
      "Test set (15000, 784) (15000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.475327\n",
      "Minibatch accuracy: 18.0%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 500: 2.839894\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1000: 1.565041\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1500: 1.133115\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 0.986383\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.696246\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.706310\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fa55255dcf2f>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1pSkLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3227f1080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 668.588013\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 23.8%\n",
      "Minibatch loss at step 500: 193.061661\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 113.657249\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 68.461411\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 41.316238\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2500: 25.170029\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3000: 15.435143\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvIQGulJKeQkNdKWU8hL/HwcHNxOV0O/4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3221f9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 370.972351\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 2: 952.214539\n",
      "Minibatch accuracy: 36.7%\n",
      "Validation accuracy: 40.9%\n",
      "Minibatch loss at step 4: 344.697205\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 53.6%\n",
      "Minibatch loss at step 6: 10.702987\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 58.9%\n",
      "Minibatch loss at step 8: 5.507030\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 58.8%\n",
      "Minibatch loss at step 10: 6.428697\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.7%\n",
      "Test accuracy: 66.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 522.686890\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 2: 814.265259\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 4: 300.448486\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 54.8%\n",
      "Minibatch loss at step 6: 24.725126\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 8: 2.053320\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 10: 26.185101\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 12: 74.086914\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 14: 16.961481\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 16: 0.000043\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 18: 1.931412\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 20: 3.458273\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 22: 0.269873\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 24: 6.727062\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 26: 1.342917\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 28: 3.533568\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 30: 2.286844\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 32: 0.303651\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 36: 0.939636\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 40: 0.764459\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 42: 0.769536\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 44: 2.977767\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 50: 3.058991\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 52: 0.909829\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 56: 1.273322\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 58: 2.320458\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 60: 0.501477\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 64: 0.263712\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 68: 1.202995\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 70: 1.206359\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 76: 1.197501\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 78: 1.862640\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 80: 0.206966\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 82: 0.051613\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 92: 0.589971\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 94: 1.581323\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 96: 2.395708\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Test accuracy: 73.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.272147\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 500: 0.930104\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000: 0.904542\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.575127\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.520965\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.531228\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.565390\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3500: 0.573201\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.445847\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4500: 0.444020\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.498980\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5500: 0.493428\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 6000: 0.563357\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.390322\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.506404\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.472213\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.571431\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8500: 0.409382\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.470708\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.427155\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.4%\n",
      "Minibatch loss at step 500: 0.363047\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.466222\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1500: 0.249981\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 0.246187\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2500: 0.279155\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.340918\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.344907\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.252765\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.248396\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5000: 0.309714\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5500: 0.205931\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.344032\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.167668\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.291468\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7500: 0.183530\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.275425\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.143154\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.174426\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9500: 0.191256\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10000: 0.177660\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10500: 0.156403\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11000: 0.076319\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11500: 0.141267\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.126884\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12500: 0.100883\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 13000: 0.167142\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13500: 0.077268\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 0.110395\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14500: 0.094239\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 15000: 0.073499\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15500: 0.083872\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.030450\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 16500: 0.058453\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17000: 0.026212\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17500: 0.013765\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 18000: 0.056984\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.9%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.644048\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 22.9%\n",
      "Minibatch loss at step 500: 0.505960\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1000: 0.571871\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1500: 0.519562\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2000: 0.388242\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2500: 0.469020\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3000: 0.533019\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3500: 0.550292\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4000: 0.479638\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4500: 0.430816\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.413097\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.483560\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6000: 0.562747\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6500: 0.346888\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7000: 0.523011\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7500: 0.518974\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 8000: 0.692198\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 8500: 0.438252\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 9000: 0.436238\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.430096\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 10000: 0.506851\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10500: 0.352449\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 11000: 0.386867\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 11500: 0.369807\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12000: 0.622503\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 12500: 0.330038\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13000: 0.437459\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13500: 0.383894\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 14000: 0.422878\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 14500: 0.470360\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15000: 0.400381\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 15500: 0.422781\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16000: 0.276475\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16500: 0.233879\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17000: 0.289002\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17500: 0.200542\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.277440\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 18500: 0.352895\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19000: 0.293568\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 19500: 0.369922\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 20000: 0.426287\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
